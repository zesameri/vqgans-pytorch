{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kE11hb9pAFrA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1KZ3HfrAFrC"
      },
      "source": [
        "Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tqVxMtQNAFrD"
      },
      "outputs": [],
      "source": [
        "class GroupNorm(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(GroupNorm, self).__init__()\n",
        "        self.gn = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gn(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ai3SzJSmAFrD"
      },
      "outputs": [],
      "source": [
        "class Swish(nn.Module):\n",
        "    \"\"\"Alternative to ReLU activation\"\"\"\n",
        "    def forward(self, x):\n",
        "        return x* torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bIbAeuqoAFrE"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.block = nn.Sequential(\n",
        "            GroupNorm(in_channels),\n",
        "            Swish(),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            GroupNorm(out_channels),\n",
        "            Swish(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.channel_up = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.in_channels != self.out_channels:\n",
        "            return self.channel_up(x) + self.block(x)\n",
        "        else:\n",
        "            return x + self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J1ZNW6NkAFrE"
      },
      "outputs": [],
      "source": [
        "class UpSampleBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(UpSampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Output is double the size of the input\n",
        "        x = F.interpolate(x, scale_factor=2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E2jw_fVlAFrF"
      },
      "outputs": [],
      "source": [
        "class DownSampleBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(DownSampleBlock, self).__init__()\n",
        "        # stride of 2 down samples\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, 2, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (0 , 1, 0, 1)\n",
        "        # pad so it is not off by one\n",
        "        x = F.pad(x, pad, mode=\"constant\", value=0)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vP77iMYwAFrF"
      },
      "outputs": [],
      "source": [
        "class NonLocalBlock(nn.Module):\n",
        "    \"\"\" Attention Block \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.in_channels = channels\n",
        "\n",
        "        self.gn = GroupNorm(channels)\n",
        "        # linear transformations\n",
        "        self.q = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.k = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.v = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.proj_out = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize input\n",
        "        h_ = self.gn(x)\n",
        "        # transform input into matrices\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        b, c, h, w = q.shape\n",
        "\n",
        "        # reshape the matrices for multiplication\n",
        "        q = q.reshape(b, c, h*w)\n",
        "        q = q.permute(0, 2, 1)\n",
        "        k = k.reshape(b, c, h*w)\n",
        "        v = v.reshape(b, c, h*w)\n",
        "\n",
        "        # attention funciton \n",
        "        attn = torch.bmm(q, k)\n",
        "        attn = attn * (int(c)**(-0.5))\n",
        "        attn = F.softmax(attn, dim=2)\n",
        "        attn = attn.permute(0, 2, 1)\n",
        "\n",
        "        A = torch.bmm(v, attn)\n",
        "        A = A.reshape(b, c, h, w)\n",
        "\n",
        "        return x + A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsT11DxsAFrG"
      },
      "source": [
        "Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H3uJULOlAFrG"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "        # channel sizes scale with residual blocks up to 512\n",
        "        channels = [128, 128, 128, 256, 256, 512]\n",
        "        # downscaling, resolution size is much smaller\n",
        "        attn_resolutions = [16]\n",
        "        # number of resnet blocks at each stage, how big the encoder will get\n",
        "        num_res_blocks = 2\n",
        "        # image size\n",
        "        resolution = 256\n",
        "\n",
        "\n",
        "        layers = [nn.Conv2d(args.image_channels, channels[0], 3, 1, 1)]\n",
        "        for i in range(len(channels)-1):\n",
        "            in_channels = channels[i]\n",
        "            out_channels = channels[i + 1]\n",
        "            for j in range(num_res_blocks):\n",
        "                layers.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "                if resolution in attn_resolutions:\n",
        "                    layers.append(NonLocalBlock(in_channels))\n",
        "            if i != len(channels)-2:\n",
        "                # decrease resolution by half\n",
        "                layers.append(DownSampleBlock(channels[i+1]))\n",
        "                resolution //= 2\n",
        "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
        "        layers.append(NonLocalBlock(channels[-1]))\n",
        "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
        "        layers.append(GroupNorm(channels[-1]))\n",
        "        layers.append(Swish())\n",
        "        layers.append(nn.Conv2d(channels[-1], args.latent_dim, 3, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LzSu8fwqAFrI"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "        channels = [512, 256, 256, 128, 128]\n",
        "        attn_resolutions = [16]\n",
        "        num_res_blocks = 3\n",
        "        resolution = 16\n",
        "\n",
        "        in_channels = channels[0]\n",
        "        layers = [nn.Conv2d(args.latent_dim, in_channels, 3, 1, 1),\n",
        "                  ResidualBlock(in_channels, in_channels),\n",
        "                  NonLocalBlock(in_channels),\n",
        "                  ResidualBlock(in_channels, in_channels)]\n",
        "\n",
        "        for i in range(len(channels)):\n",
        "            out_channels = channels[i]\n",
        "            for j in range(num_res_blocks):\n",
        "                layers.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "                if resolution in attn_resolutions:\n",
        "                    layers.append(NonLocalBlock(in_channels))\n",
        "            if i != 0:\n",
        "                layers.append(UpSampleBlock(in_channels))\n",
        "                resolution *= 2\n",
        "\n",
        "        layers.append(GroupNorm(in_channels))\n",
        "        layers.append(Swish())\n",
        "        layers.append(nn.Conv2d(in_channels, args.image_channels, 3, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6SHQQKKtAFrI"
      },
      "outputs": [],
      "source": [
        "class Codebook(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Codebook, self).__init__()\n",
        "        # Number of codebook vectors\n",
        "        self.num_codebook_vectors = args.num_codebook_vectors\n",
        "        self.latent_dim = args.latent_dim\n",
        "        # weighting factor for codebook loss\n",
        "        self.beta = args.beta\n",
        "\n",
        "        # embedding matrix (data structure of the codebook)\n",
        "        self.embedding = nn.Embedding(self.num_codebook_vectors, self.latent_dim)\n",
        "        # initialize weights by uniform distribution\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.num_codebook_vectors, 1.0 / self.num_codebook_vectors)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"Find min distance between codebook vectors and latent space\"\"\"\n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        # flatten the latent vectors\n",
        "        z_flattened = z.view(-1, self.latent_dim)\n",
        "\n",
        "        # distance between latent and codebook vectors\n",
        "        # (a - b)^2 = a^2 - 2ab + b^2\n",
        "        d = torch.sum(z_flattened**2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - \\\n",
        "            2*(torch.matmul(z_flattened, self.embedding.weight.t()))\n",
        "\n",
        "        # look at first axis for closest vectors\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        # this is the set of vectors from the codebook that are closest to the latent vectors\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # get codebook loss, the stop gradient function/detatch function\n",
        "        loss = torch.mean((z_q.detach() - z)**2) + self.beta * torch.mean((z_q - z.detach())**2)\n",
        "\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        z_q = z_q.permute(0, 3, 1, 2)\n",
        "\n",
        "        return z_q, min_encoding_indices, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjniAlbRAFrJ"
      },
      "source": [
        "VQGans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Fr2i6aoUAFrJ"
      },
      "outputs": [],
      "source": [
        "class VQGAN(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(VQGAN, self).__init__()\n",
        "        self.encoder = Encoder(args).to(device=args.device)\n",
        "        self.decoder = Decoder(args).to(device=args.device)\n",
        "        self.codebook = Codebook(args).to(device=args.device)\n",
        "        self.pre_quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
        "        self.post_quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        encoded_images = self.encoder(imgs)\n",
        "        pre_quant_conv_encoded_images = self.pre_quant_conv(encoded_images)\n",
        "        # codebook is doing the quantization\n",
        "        codebook_mapping, codebook_indices, q_loss = self.codebook(pre_quant_conv_encoded_images)\n",
        "        post_quant_conv_mapping = self.post_quant_conv(codebook_mapping)\n",
        "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
        "\n",
        "        return decoded_images, codebook_indices, q_loss\n",
        "\n",
        "    # for the transformer\n",
        "    def encode(self, imgs):\n",
        "        encoded_images = self.encoder(imgs)\n",
        "        pre_quant_conv_encoded_images = self.pre_quant_conv(encoded_images)\n",
        "        codebook_mapping, codebook_indices, q_loss = self.codebook(pre_quant_conv_encoded_images)\n",
        "        return codebook_mapping, codebook_indices, q_loss\n",
        "\n",
        "    # for the transformer\n",
        "    def decode(self, z):\n",
        "        post_quant_conv_mapping = self.post_quant_conv(z)\n",
        "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
        "        return decoded_images\n",
        "\n",
        "    def calculate_lambda(self, perceptual_loss, gan_loss):\n",
        "        # get ratio, gradient of last layer of decoder's loss, over the perceptual & gan loss\n",
        "        last_layer = self.decoder.model[-1]\n",
        "        last_layer_weight = last_layer.weight\n",
        "        perceptual_loss_grads = torch.autograd.grad(perceptual_loss, last_layer_weight, retain_graph=True)[0]\n",
        "        gan_loss_grads = torch.autograd.grad(gan_loss, last_layer_weight, retain_graph=True)[0]\n",
        "\n",
        "        λ = torch.norm(perceptual_loss_grads) / (torch.norm(gan_loss_grads) + 1e-4)\n",
        "        # clamp between 0 and 10,000\n",
        "        λ = torch.clamp(λ, 0, 1e4).detach()\n",
        "        return 0.8 * λ\n",
        "\n",
        "    @staticmethod\n",
        "    def adopt_weight(disc_factor, i, threshold, value=0.):\n",
        "        \"\"\"We want to have the discriminator start later so that the transformer has time to learn how to create images. threshold acts as start time\"\"\"\n",
        "        if i < threshold:\n",
        "            disc_factor = value\n",
        "        return disc_factor\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        # load the checkpoint of the vqgan\n",
        "        self.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTU-p0ASAFrK"
      },
      "source": [
        "Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_NOBiVXjAFrL"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, args, num_filters_last=64, n_layers=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        layers = [nn.Conv2d(args.image_channels, num_filters_last, 4, 2, 1), nn.LeakyReLU(0.2)]\n",
        "        num_filters_mult = 1\n",
        "\n",
        "        for i in range(1, n_layers + 1):\n",
        "            num_filters_mult_last = num_filters_mult\n",
        "            num_filters_mult = min(2 ** i, 8)\n",
        "            layers += [\n",
        "                nn.Conv2d(num_filters_last * num_filters_mult_last, num_filters_last * num_filters_mult, 4,\n",
        "                          2 if i < n_layers else 1, 1, bias=False),\n",
        "                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "        \n",
        "        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ablpj0C-AFrM"
      },
      "source": [
        "LPIPS (Perceptual Loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WHyOppmdAFrM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.models import vgg16\n",
        "from collections import namedtuple\n",
        "import requests\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kLLmARXvAFrN"
      },
      "outputs": [],
      "source": [
        "URL_MAP = {\n",
        "    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n",
        "}\n",
        "\n",
        "CKPT_MAP = {\n",
        "    \"vgg_lpips\": \"vgg.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "def download(url, local_path, chunk_size=1024):\n",
        "    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        total_size = int(r.headers.get(\"content-length\", 0))\n",
        "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n",
        "            with open(local_path, \"wb\") as f:\n",
        "                for data in r.iter_content(chunk_size=chunk_size):\n",
        "                    if data:\n",
        "                        f.write(data)\n",
        "                        pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def get_ckpt_path(name, root):\n",
        "    assert name in URL_MAP\n",
        "    path = os.path.join(root, CKPT_MAP[name])\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Downloading {name} model from {URL_MAP[name]} to {path}\")\n",
        "        download(URL_MAP[name], path)\n",
        "    return path\n",
        "\n",
        "\n",
        "class LPIPS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LPIPS, self).__init__()\n",
        "        self.scaling_layer = ScalingLayer()\n",
        "        self.channels = [64, 128, 256, 512, 512]\n",
        "        self.vgg = VGG16()\n",
        "        self.lins = nn.ModuleList([\n",
        "            NetLinLayer(self.channels[0]),\n",
        "            NetLinLayer(self.channels[1]),\n",
        "            NetLinLayer(self.channels[2]),\n",
        "            NetLinLayer(self.channels[3]),\n",
        "            NetLinLayer(self.channels[4])\n",
        "        ])\n",
        "\n",
        "        self.load_from_pretrained()\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def load_from_pretrained(self, name=\"vgg_lpips\"):\n",
        "        ckpt = get_ckpt_path(name, \"vgg_lpips\")\n",
        "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
        "\n",
        "    def forward(self, real_x, fake_x):\n",
        "        features_real = self.vgg(self.scaling_layer(real_x))\n",
        "        features_fake = self.vgg(self.scaling_layer(fake_x))\n",
        "        diffs = {}\n",
        "\n",
        "        for i in range(len(self.channels)):\n",
        "            diffs[i] = (norm_tensor(features_real[i]) - norm_tensor(features_fake[i])) ** 2\n",
        "\n",
        "        return sum([spatial_average(self.lins[i].model(diffs[i])) for i in range(len(self.channels))])\n",
        "\n",
        "\n",
        "class ScalingLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScalingLayer, self).__init__()\n",
        "        self.register_buffer(\"shift\", torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
        "        self.register_buffer(\"scale\", torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x - self.shift) / self.scale\n",
        "\n",
        "\n",
        "class NetLinLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=1):\n",
        "        super(NetLinLayer, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
        "        )\n",
        "\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        vgg_pretrained_features = vgg16(pretrained=True).features\n",
        "        slices = [vgg_pretrained_features[i] for i in range(30)]\n",
        "        self.slice1 = nn.Sequential(*slices[0:4])\n",
        "        self.slice2 = nn.Sequential(*slices[4:9])\n",
        "        self.slice3 = nn.Sequential(*slices[9:16])\n",
        "        self.slice4 = nn.Sequential(*slices[16:23])\n",
        "        self.slice5 = nn.Sequential(*slices[23:30])\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.slice1(x)\n",
        "        h_relu1 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4 = h\n",
        "        h = self.slice5(h)\n",
        "        h_relu5 = h\n",
        "        vgg_outputs = namedtuple(\"VGGOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
        "        return vgg_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n",
        "\n",
        "\n",
        "def norm_tensor(x):\n",
        "    \"\"\"\n",
        "    Normalize images by their length to make them unit vector?\n",
        "    :param x: batch of images\n",
        "    :return: normalized batch of images\n",
        "    \"\"\"\n",
        "    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
        "    return x / (norm_factor + 1e-10)\n",
        "\n",
        "\n",
        "def spatial_average(x):\n",
        "    \"\"\"\n",
        "     imgs have: batch_size x channels x width x height --> average over width and height channel\n",
        "    :param x: batch of images\n",
        "    :return: averaged images along width and height\n",
        "    \"\"\"\n",
        "    return x.mean([2, 3], keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHG8gQsWAFrO"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d1VzTKwNAFrO"
      },
      "outputs": [],
      "source": [
        "import albumentations\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RhfQ49_SAFrP"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------- #\n",
        "#                  Data Utils\n",
        "# --------------------------------------------- #\n",
        "\n",
        "class ImagePaths(Dataset):\n",
        "    \"\"\" How we load the data \"\"\"\n",
        "    def __init__(self, path, size=None):\n",
        "        self.size = size\n",
        "\n",
        "        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n",
        "        self._length = len(self.images)\n",
        "\n",
        "        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n",
        "        self.cropper = albumentations.CenterCrop(height=self.size, width=self.size)\n",
        "        self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        \"\"\" Convert to RGB, resize image, center crop\"\"\"\n",
        "        image = Image.open(image_path)\n",
        "        if not image.mode == \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        image = np.array(image).astype(np.uint8)\n",
        "        image = self.preprocessor(image=image)[\"image\"]\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        example = self.preprocess_image(self.images[i])\n",
        "        return example\n",
        "\n",
        "\n",
        "def load_data(args):\n",
        "    train_data = ImagePaths(args.dataset_path, size=256)\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "# --------------------------------------------- #\n",
        "#                  Module Utils\n",
        "#            for Encoder, Decoder etc.\n",
        "# --------------------------------------------- #\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "def plot_images(images):\n",
        "    x = images[\"input\"]\n",
        "    reconstruction = images[\"rec\"]\n",
        "    half_sample = images[\"half_sample\"]\n",
        "    full_sample = images[\"full_sample\"]\n",
        "\n",
        "    fig, axarr = plt.subplots(1, 4)\n",
        "    axarr[0].imshow(x.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[1].imshow(reconstruction.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[2].imshow(half_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[3].imshow(full_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0GAZkHOAFrP"
      },
      "source": [
        "Training the VQGans Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o8g0soynAFrQ"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from torchvision import utils as vutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jsto_VFAFrR"
      },
      "outputs": [],
      "source": [
        "class TrainVQGAN:\n",
        "    def __init__(self, args):\n",
        "        self.vqgan = VQGAN(args).to(device=args.device)\n",
        "        self.discriminator = Discriminator(args).to(device=args.device)\n",
        "        self.discriminator.apply(weights_init)\n",
        "        self.perceptual_loss = LPIPS().eval().to(device=args.device)\n",
        "        # optimizers\n",
        "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
        "\n",
        "        # set up directories\n",
        "        self.prepare_training()\n",
        "\n",
        "        self.train(args)\n",
        "\n",
        "    def configure_optimizers(self, args):\n",
        "        lr = args.learning_rate\n",
        "        opt_vq = torch.optim.Adam(\n",
        "            list(self.vqgan.encoder.parameters()) +\n",
        "            list(self.vqgan.decoder.parameters()) +\n",
        "            list(self.vqgan.codebook.parameters()) +\n",
        "            list(self.vqgan.quant_conv.parameters()) +\n",
        "            list(self.vqgan.post_quant_conv.parameters()),\n",
        "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2)\n",
        "        )\n",
        "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
        "                                    lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
        "\n",
        "        return opt_vq, opt_disc\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_training():\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    def train(self, args):\n",
        "        train_dataset = load_data(args)\n",
        "        steps_per_epoch = len(train_dataset)\n",
        "        for epoch in range(args.epochs):\n",
        "            with tqdm(range(len(train_dataset))) as pbar:\n",
        "                for i, imgs in zip(pbar, train_dataset):\n",
        "                    imgs = imgs.to(device=args.device)\n",
        "                    decoded_images, _, q_loss = self.vqgan(imgs)\n",
        "\n",
        "                    disc_real = self.discriminator(imgs)\n",
        "                    disc_fake = self.discriminator(decoded_images)\n",
        "\n",
        "                    disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch*steps_per_epoch+i, threshold=args.disc_start)\n",
        "\n",
        "                    perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
        "                    # L1 Loss\n",
        "                    rec_loss = torch.abs(imgs - decoded_images)\n",
        "                    perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n",
        "                    perceptual_rec_loss = perceptual_rec_loss.mean()\n",
        "                    # Discriminator loss for generator\n",
        "                    g_loss = -torch.mean(disc_fake)\n",
        "\n",
        "                    λ = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
        "                    vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n",
        "\n",
        "                    d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
        "                    d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
        "                    gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n",
        "\n",
        "                    self.opt_vq.zero_grad()\n",
        "                    vq_loss.backward(retain_graph=True)\n",
        "\n",
        "                    self.opt_disc.zero_grad()\n",
        "                    # what does backward the loss mean?\n",
        "                    gan_loss.backward()\n",
        "\n",
        "                    self.opt_vq.step()\n",
        "                    self.opt_disc.step()\n",
        "\n",
        "                    if i % 10 == 0:\n",
        "                        with torch.no_grad():\n",
        "                            real_fake_images = torch.cat((imgs[:4], decoded_images.add(1).mul(0.5)[:4]))\n",
        "                            vutils.save_image(real_fake_images, os.path.join(\"results\", f\"{epoch}_{i}.jpg\"), nrow=4)\n",
        "\n",
        "                    pbar.set_postfix(\n",
        "                        VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
        "                        GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
        "                    )\n",
        "                    pbar.update(0)\n",
        "                torch.save(self.vqgan.state_dict(), os.path.join(\"checkpoints\", f\"vqgan_epoch_{epoch}.pt\"))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
        "    parser.add_argument('-f')\n",
        "    parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
        "    parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
        "    parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
        "    parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
        "    parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
        "    parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
        "    parser.add_argument('--batch-size', type=int, default=6, help='Input batch size for training (default: 6)')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
        "    parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
        "    parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator (default: 0)')\n",
        "    parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
        "    parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
        "    parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    args.dataset_path = r\"C:\\Users\\dome\\datasets\\flowers\"\n",
        "\n",
        "    train_vqgan = TrainVQGAN(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTbA1joVAFrS"
      },
      "source": [
        "Min GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BqAke2PGAFrT"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1bUAU411AFrT"
      },
      "outputs": [],
      "source": [
        "class GPTConfig:\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mMPdtijQAFrT"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        mask = torch.tril(torch.ones(config.block_size,\n",
        "                                     config.block_size))\n",
        "        if hasattr(config, \"n_unmasked\"):\n",
        "            mask[:config.n_unmasked, :config.n_unmasked] = 1\n",
        "        self.register_buffer(\"mask\", mask.view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        present = torch.stack((k, v))\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past\n",
        "            k = torch.cat((past_key, k), dim=-2)\n",
        "            v = torch.cat((past_value, v), dim=-2)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        if layer_past is None:\n",
        "            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y, present  # TODO: check that this does not break anything\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lllkLU7GAFrT"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),  # nice\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, layer_past=None, return_present=False):\n",
        "        # TODO: check that training still works\n",
        "        if return_present:\n",
        "            assert not self.training\n",
        "        # layer past: tuple of length two with B, nh, T, hs\n",
        "        attn, present = self.attn(self.ln1(x), layer_past=layer_past)\n",
        "\n",
        "        x = x + attn\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        if layer_past is not None or return_present:\n",
        "            return x, present\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ck1TsqCoAFrU"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, n_layer=12, n_head=8, n_embd=256,\n",
        "                 embd_pdrop=0., resid_pdrop=0., attn_pdrop=0., n_unmasked=0):\n",
        "        super().__init__()\n",
        "        config = GPTConfig(vocab_size=vocab_size, block_size=block_size,\n",
        "                           embd_pdrop=embd_pdrop, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop,\n",
        "                           n_layer=n_layer, n_head=n_head, n_embd=n_embd,\n",
        "                           n_unmasked=n_unmasked)\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))  # 512 x 1024\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "        self.config = config\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, idx, embeddings=None):\n",
        "        token_embeddings = self.tok_emb(idx)  # each index maps to a (learnable) vector\n",
        "\n",
        "        if embeddings is not None:  # prepend explicit embeddings\n",
        "            token_embeddings = torch.cat((embeddings, token_embeddings), dim=1)\n",
        "\n",
        "        t = token_embeddings.shape[1]\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        position_embeddings = self.pos_emb[:, :t, :]  # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        return logits, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIpU6fDlAFrU"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ns_JC5ViAFrU"
      },
      "outputs": [],
      "source": [
        "class VQGANTransformer(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(VQGANTransformer, self).__init__()\n",
        "\n",
        "        self.sos_token = args.sos_token\n",
        "\n",
        "        self.vqgan = self.load_vqgan(args)\n",
        "\n",
        "        transformer_config = {\n",
        "            \"vocab_size\": args.num_codebook_vectors,\n",
        "            \"block_size\": 512,\n",
        "            \"n_layer\": 24,\n",
        "            \"n_head\": 16,\n",
        "            \"n_embd\": 1024\n",
        "        }\n",
        "        self.transformer = GPT(**transformer_config)\n",
        "\n",
        "        self.pkeep = args.pkeep\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vqgan(args):\n",
        "        model = VQGAN(args)\n",
        "        model.load_checkpoint(args.checkpoint_path)\n",
        "        model = model.eval()\n",
        "        return model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_to_z(self, x):\n",
        "        quant_z, indices, _ = self.vqgan.encode(x)\n",
        "        indices = indices.view(quant_z.shape[0], -1)\n",
        "        return quant_z, indices\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def z_to_image(self, indices, p1=16, p2=16):\n",
        "        ix_to_vectors = self.vqgan.codebook.embedding(indices).reshape(indices.shape[0], p1, p2, 256)\n",
        "        ix_to_vectors = ix_to_vectors.permute(0, 3, 1, 2)\n",
        "        image = self.vqgan.decode(ix_to_vectors)\n",
        "        return image\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, indices = self.encode_to_z(x)\n",
        "\n",
        "        sos_tokens = torch.ones(x.shape[0], 1) * self.sos_token\n",
        "        sos_tokens = sos_tokens.long().to(\"cuda\")\n",
        "\n",
        "        mask = torch.bernoulli(self.pkeep * torch.ones(indices.shape, device=indices.device))\n",
        "        mask = mask.round().to(dtype=torch.int64)\n",
        "        random_indices = torch.randint_like(indices, self.transformer.config.vocab_size)\n",
        "        new_indices = mask * indices + (1 - mask) * random_indices\n",
        "\n",
        "        new_indices = torch.cat((sos_tokens, new_indices), dim=1)\n",
        "\n",
        "        target = indices\n",
        "\n",
        "        logits, _ = self.transformer(new_indices[:, :-1])\n",
        "\n",
        "        return logits, target\n",
        "\n",
        "    def top_k_logits(self, logits, k):\n",
        "        v, ix = torch.topk(logits, k)\n",
        "        out = logits.clone()\n",
        "        out[out < v[..., [-1]]] = -float(\"inf\")\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, x, c, steps, temperature=1.0, top_k=100):\n",
        "        self.transformer.eval()\n",
        "        x = torch.cat((c, x), dim=1)\n",
        "        for k in range(steps):\n",
        "            logits, _ = self.transformer(x)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                logits = self.top_k_logits(logits, top_k)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "        x = x[:, c.shape[1]:]\n",
        "        self.transformer.train()\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, x):\n",
        "        log = dict()\n",
        "\n",
        "        _, indices = self.encode_to_z(x)\n",
        "        sos_tokens = torch.ones(x.shape[0], 1) * self.sos_token\n",
        "        sos_tokens = sos_tokens.long().to(\"cuda\")\n",
        "\n",
        "        start_indices = indices[:, :indices.shape[1] // 2]\n",
        "        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1] - start_indices.shape[1])\n",
        "        half_sample = self.z_to_image(sample_indices)\n",
        "\n",
        "        start_indices = indices[:, :0]\n",
        "        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1])\n",
        "        full_sample = self.z_to_image(sample_indices)\n",
        "\n",
        "        x_rec = self.z_to_image(indices)\n",
        "\n",
        "        log[\"input\"] = x\n",
        "        log[\"rec\"] = x_rec\n",
        "        log[\"half_sample\"] = half_sample\n",
        "        log[\"full_sample\"] = full_sample\n",
        "\n",
        "        return log, torch.concat((x, x_rec, half_sample, full_sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Transformer"
      ],
      "metadata": {
        "id": "Ifg3e7qiBMs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainTransformer:\n",
        "    def __init__(self, args):\n",
        "        self.model = VQGANTransformer(args).to(device=args.device)\n",
        "        self.optim = self.configure_optimizers()\n",
        "\n",
        "        self.train(args)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        decay, no_decay = set(), set()\n",
        "        whitelist_weight_modules = (nn.Linear, )\n",
        "        blacklist_weight_modules = (nn.LayerNorm, nn.Embedding)\n",
        "\n",
        "        for mn, m in self.model.transformer.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = f\"{mn}.{pn}\" if mn else pn\n",
        "\n",
        "                if pn.endswith(\"bias\"):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "                elif pn.endswith(\"weight\") and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "\n",
        "                elif pn.endswith(\"weight\") and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        no_decay.add(\"pos_emb\")\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.model.transformer.named_parameters()}\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": 0.01},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=4.5e-06, betas=(0.9, 0.95))\n",
        "        return optimizer\n",
        "\n",
        "    def train(self, args):\n",
        "        train_dataset = load_data(args)\n",
        "        for epoch in range(args.epochs):\n",
        "            with tqdm(range(len(train_dataset))) as pbar:\n",
        "                for i, imgs in zip(pbar, train_dataset):\n",
        "                    self.optim.zero_grad()\n",
        "                    imgs = imgs.to(device=args.device)\n",
        "                    logits, targets = self.model(imgs)\n",
        "                    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "                    loss.backward()\n",
        "                    self.optim.step()\n",
        "                    pbar.set_postfix(Transformer_Loss=np.round(loss.cpu().detach().numpy().item(), 4))\n",
        "                    pbar.update(0)\n",
        "            log, sampled_imgs = self.model.log_images(imgs[0][None])\n",
        "            vutils.save_image(sampled_imgs, os.path.join(\"results\", f\"transformer_{epoch}.jpg\"), nrow=4)\n",
        "            plot_images(log)\n",
        "            torch.save(self.model.state_dict(), os.path.join(\"checkpoints\", f\"transformer_{epoch}.pt\"))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
        "    parser.add_argument('-f')\n",
        "    parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z.')\n",
        "    parser.add_argument('--image-size', type=int, default=256, help='Image height and width.)')\n",
        "    parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors.')\n",
        "    parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar.')\n",
        "    parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images.')\n",
        "    parser.add_argument('--dataset-path', type=str, default='./data', help='Path to data.')\n",
        "    parser.add_argument('--checkpoint-path', type=str, default='./checkpoints/last_ckpt.pt', help='Path to checkpoint.')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
        "    parser.add_argument('--batch-size', type=int, default=20, help='Input batch size for training.')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
        "    parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate.')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param.')\n",
        "    parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param.')\n",
        "    parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator.')\n",
        "    parser.add_argument('--disc-factor', type=float, default=1., help='Weighting factor for the Discriminator.')\n",
        "    parser.add_argument('--l2-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
        "    parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
        "\n",
        "    parser.add_argument('--pkeep', type=float, default=0.5, help='Percentage for how much latent codes to keep.')\n",
        "    parser.add_argument('--sos-token', type=int, default=0, help='Start of Sentence token.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    args.dataset_path = r\"C:\\Users\\dome\\datasets\\flowers\"\n",
        "    args.checkpoint_path = r\".\\checkpoints\\vqgan_last_ckpt.pt\"\n",
        "\n",
        "    train_transformer = TrainTransformer(args)"
      ],
      "metadata": {
        "id": "azm1GqCuBO2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3f3dfd57afce7d6a2cf81d67a2ad1910b7d59df7bbe1b67ffe7a0f9db1fb8d54"
      }
    },
    "colab": {
      "name": "vqgans.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}